% Chapter 2

\chapter{Translation, translators and translating} % Chapter title
\label{ch:translation} % For referencing the chapter elsewhere, use \autoref{ch:examples} 

%----------------------------------------------------------------------------------------

\noindent Before delving into \ac{CAT} tools, it is best to start from the beginning and discuss what translation is. General theories of translation are estranged in \ac{MT} research. Other areas of research in \ac{NLP} base their research on theories ---~commonly linguistic theories~---, upon which models are built, implemented and tested. \ac{MT} and \ac{CAT} systems today are not explicitely built on any general theory of translation.

In this chapter, some of the main theories of translation are discussed for two reasons. First and foremost, a strong theoretical basis will reinforce research into \ac{MT}, \ac{CAT} and related technologies as well as provide a better idea of where they should be heading and what can be improved. Having an explicit starting point for research can clear the way for competing theories to emerge and develop their own lines of research if considered necessary. 

Second, \acf{TPR} research (\autoref{sec:tpr}) ---~part of theoretical research into translation~--- provides a useful set of methodological tools. These tools will be used in this thesis and integrated into the proposed research methodology (\autoref{ch:methodology}).

% TODO Get rid of the following paragraph?

%Also, it is worth noting that the output of \ac{MT} systems has improved considerably in recent years, leading to a focus on attaining ``quality'' translations as the next step up from gisting systems. Defining quality, measuring it and creating metrics useful for \ac{MT}, \ac{PE} or even \ac{HT} is an active and ongoing field of research. It is thus natural to turn to \acf{TS} and the theories it has developed ---~which have been dealing with the notion of ``quality'' and the very essence of what translation is for a long time~--- to see if answers that jointly describe \ac{MT} and \ac{HT} can be found.

\section{Overview of paradigms in Translation Theory}
\label{sec:paradigms}

\noindent This section provides a high-level overview of the main paradigms in translation theory. They are taken as per \textcite{pym2009exploring}, who describes five basic paradigms. Together they cover most of the individual theories of translation that have been proposed over the years. 

\subsection{Theories of Equivalence}

\noindent The equivalence paradigm postulates that there is an equalness of value between the \ac{ST} and the \ac{TT} at some level. This paradigm relies heavily on linguistics, as can be seen in the work of \textcite{catford1965linguistic}, who postulated different possible equivalence levels for a \ac{ST} and a \ac{TT}: phonetic, lexical, syntactic, etc.

More generally, the focus in the theories of equivalence is placed on the \ac{ST}, which guides and establishes what will appear in the \ac{TT}. Translation is thus understood in the sense of creating a \ac{TT} which is equivalent to the \ac{ST} and contains nothing but the \ac{ST} (save clearly delimited translator's notes). This is the notion that is the most widespread amongst the general public and \ac{MT} research.

\subsection{Skopos or Functional Theories}

\noindent Skopos theory (from the Greek \textit{skopos} `aim', `intention') introduced a radically new idea in translation: what matters when translating is the purpose, the function that the \ac{TT} has to fulfil \parencite{holz1984translatorisches,reiss1984grundlegung}. This theory escaped from the constraints of linguistics and the \ac{ST} to enter the real world, where translators carry out translations according to customers' requirements. Large variations in the \ac{TT} are acceptable if justified by the function that the \ac{TT} has to fulfil, which is often different from that of the \ac{ST}. Equivalence is thus relagated to the case where the function is the same for the \ac{ST} and the \ac{TT}. 

One of the most profound implications of this theory is that the reasons for choosing to translate a \ac{ST} in one way or another cannot be found in linguistics, they are found in communication, ethics, sociology, marketing, etc. This lead to the appearance of agents in the paradigm, \ie the customer, the employer, users of the translation, etc. For some theoreticians, the main agents in this paradigm are the translators, since they are the ones who have the final word on decisions that have to be made and other agents simply collaborate in this task \parencite{reiss1984grundlegung}. For others, the focus is more on the client's requirements and not on the translator's individuality \parencite{nord1997translation}.

\subsection{Descriptive Theories}
\label{sub:descriptive}

\noindent Descriptive Translation Studies, developed in parallel to skopos theories, again presented a breakwaway from previous theories. In this paradigm the notion of ``assumed translations'' \parencite{toury1995notion} takes centre stage: all translations are equivalent due to them being considered a translation. Thus, the aim is to observe originals and translations and describe how they differ and what makes them equivalent rather than prescribing a certain type of equivalence or a need to adapt to the \ac{TT} function as the only valid ways to translate.

One of the main notions in the descriptive paradigm is that of norms, whose main proponent is \textcite{toury1995nature}. Translations are seen to be produced within a certain culture and society, and at a certain time in history. Norms turn the ideas and values within a society into ``performance instructions'', \ie\ what can or cannot be done, what is permissable and what isn't \parencite{toury1995nature}. Translators will thus adopt different strategies and produce different translations according to the society and historical time they live in. The notion of norms can lead to prescriptivisim: discovering the relevant norms allows for translations to be classified as good or bad depending on whether they follow the norms of a certain time and society. Thus, the notion of norms can lead back to the presciptivism it was trying to escape. 

\subsection{Theories of Indeterminacy}

\noindent The paradigm of indeterminism questions many of the assumptions of the other paradigms, and even the existance of translation itself. The principle of indeterminacy \parencite{quine1969linguistics} postulated that it is impossible to know the real meaning of an utterance, there are always other possible interpretations based on the same empirical evidence. In translation, this led to the view that there are always many possible ways to translate a text and it is impossible to be certain that any one of them is right or is equivalent to the ``meaning'' of the \ac{ST}. We can always construct hypotheses to defend one interpretation or another, all of which can be supported by the same empirical evidence, \ie the \ac{ST} \parencite{quine1969linguistics}.

\textcite{pym2009exploring} groups various related but somewhat different theories in this paradigm. A full overview is not within the scope of this thesis, but the notion of ``abusive fidelity'' \parencite{lewis1985measure} is one worth mentioning. One of the problems with indeterminisim is that it doesn't provide translators with ways of carrying out their work. The closest there is to a guideline is to ``abuse fidelity'' and translate the key points within a text as close as possible to the \ac{ST}, to the point of making the \ac{TT} sound strange to the target culture \parencite{lewis1985measure}. Thus, users are made aware of the translation itself by breaking the illusion of equivalence or symmetry between languages \parencite{snell1988translation}, the illusion that a universal transfer of meaning is possible.

\subsection{Localisation}
\label{sub:localisation}

\noindent \textcite{pym2009exploring} introduces and describes localisation as a new paradigm in \ac{TS}. The process of localisation ---~translating a product into different locales~--- introduces three important concepts. First, the idea of locale, which combines both a language and a specific culture. For example, Spanish can be divided into the Spain-Spanish locale, the Mexico-Spanish locale, etc. Second, internationalisation as the process of adapting a \ac{ST} so that it can be easily translated into various locales. This entails the creation of an intermediate product that is either augmented (more space for strings, adding date formatting options to software, etc.) or simplified through the use of a \ac{CNL}.

Third, \textcite{pym2009exploring} cites the rise of non-linear texts, which are often those that are localised. Non-linear texts are neither translated nor used from beginning to end. Rather, fragments are used such as the various strings in a piece of software, parts of a reference manual for software or appliances, etc. These products are usually incrementally updated, meaning that future translations are simply the new or modified segments of the product. It often occurrs that these segments are presented to translators with little to no context as to where they are to appear in the final product.

The three previous factors bring about a change in the way translators work. Translation is but a single step in a larger process consisting of internationalisation, translation, editing and quality assurance. Translation is reduced to an artificial kind of equivalence weighed down by the importance of reuse and accepting \acl{TM} suggestions or terminology from glossaries (even though translators may consider them wrong or inadequate), with little incentive for translators to want to improve it \parencite{pym2009exploring}. \textcite{esselink2002localisation} notes that translators should carry out other steps in the localisation process, such as internationalisation and final editing, for which their intercultural skills are very useful and relevant.

\section{MT: Translation as a Distributed System}
\label{sec:mt_theory}

\noindent The paradigms presented in the previous section are but a brief glimpse of all the opinions and theories that have been proposed about translation, but a detailed review of them is outside of the scope and aim of this thesis\footnote{For further details, \textcite{pym2009exploring} provides a thorough discussion of each paradigm, its authors, its pros and cons and its authors.}. What they do provide is a good basis for discussion of \ac{MT} and how it fits in and relates to the paradigms.

\textcite{pym2009exploring} places \ac{MT} in the equivalence paradigm, albeit a new kind of equivalence that serves the higher goal of reuse. Looking at the methods \ac{SMT} uses today, this would seem to be the case. A large corpus of translated texts are collected, aligned at the sentence level, broken down into phrases or syntactic structures, aligned again and probabilities are calculated. Texts to be translated are then broken down into sentences and these pieces are then reused to create the \ac{TT}. Just as was the case in the theories of equivalence, the \ac{ST} rules the roost: it is the main element that guides what appears in the \ac{TT} and linguistics is all that matters. \textcite{kay1997still} also shares this view of \ac{MT}, as the following passage ---~which still rings true today~--- shows:

\begin{quote}
    Workers in this field [\textit{\ac{MT} research}] usually take it as self-evident what constitutes a translation, especially within the relatively limited domains in which automatic methods might be applied. In particular, it is assumed that the sequence of sentences that make up the translation should preserve the intended meanings of the corresponding sentences in the original. \\
--- \textcite{kay1997still}
\end{quote}

\noindent \textcite{culo2014approaching} characterises \ac{MT} as being instrumental and aiming to be functionally constant. Thus, \ac{MT} is still stuck in the equivalence paradigm, but limited to cases where the \ac{ST} and \ac{TT} functions are the same. The problem arises when considering the corpora that \ac{SMT} systems are based on: since they will have been translated by different people, from different sources and for different purposes, can we be certain that what resulting translations will be functionally identical? What is more, statistical systems will choose the most probable translation options from this amalgama, can probability really guarantee functional equivalence? From the point of view of descriptive theories, it might well be that the most probable translations that \ac{SMT} generates correspond to translation norms present in the parallel corpus used. If, for example, a corpora of 19th century texts were fed to an \ac{MT} system, the resulting translations might make use of the translation norms of the time. But then again, the very process of breaking down sentences into phrases or syntactic structures and rebuilding them might water down the norms and generate some other \ac{SMT} intrinsic norms. Studies would be required to determine if this is the case.

If we take a wider view, we can see that \ac{MT} does indeed use skopos theory. The techniques of domain adaptation and the use of in-domain data imply that the \ac{MT} developer is thinking about the users of the \ac{TT} and modifying what the \ac{MT} system should output accordingly. Translators change their translation strategies in order to adapt the \ac{TT} to the target culture. Since changing the \ac{MT} system is costly and can involve much research and effort, the next best approach is used to adapt the \ac{TT}: use data that closely resembles it. The methods and results are different, but the aim is the same. This implies that linguistics (or its statistical proxy) is not the be-all and end-all in translation, but that external factors have to be taken into account. 

One final point to note about the previous discussion is the return to a human. Trying to understand \ac{MT} from a theoretical perspective forces us to step back from the system itself with its mathematics or rules and into the human realm where we can attribute intelligent phenomena such as intentionality and thought. In the case of \ac{MT}, it is the system developers and system implementers that have stepped into the shoes of translators and have to carry out the translation. Instead of doing it all themselves, they proxy the work to computers: a set of rules or a parallel corpus takes the place of memory, mathematics and algorithms take the place of translation strategies and procedures. This view rings close to what \textcite{pym2009exploring} describes for the localisation paradigm: splitting what a single translator used to do alone into parts that are performed by different people or even computers. In other words, translation as a distributed system. \ac{MT} is simply the expression of translation where the majority of those parts are taken over by a computer.

\section{Translation Process Research}
\label{sec:tpr}

\noindent \acf{TPR} is a theoretical branch closely related to descriptive translation studies (\autoref{sub:descriptive}) that aims to describe how translators translate. It focuses on aspects such as translation units, styles, phases, variation among translators, etc. Essentially, it provides a detailed description of how a translation is carried out by a translator, not just a description of the final text itself. 

\textcite{christensen2011studies} provides an overview of the cognitive grounding for translation and \ac{TPR}, a brief summary of which now follows. Some aspects of the translation process can be observed directly, but a large part of it occurs in a translator's mind. To understand the translation process better, we need to access the cognitive phenomena that go on in the mind. \textcite{risku2010cognitive} notes that cognition is not only a mental affair, the environment plays a direct role in the thought processes. So, understanding translation requires observing what goes on in the mind as well as what goes on in a translator's work environment. \textcite{hutchins2000distributed} proposes the Distributed Cognition Paradigm, in which cognition is distributed across people and artifacts that make humans smarter. In other words, we think with our minds and with our tools, artifacts which can change and shape our mental processes. Thus, when developing tools for translators it becomes relevant to see how they affect their mental processes, and not just the final translation.

In order to gather data on the cognitive processes that go on in the mind, several methodologies exist. \textcite{christensen2011studies} provides a detailed classification and description of the positive and negative aspects of each. A first major distinction is whether the data is collected after the translation (offline) or as the translation is being carried out (online). Within each group, two further subgroups can be defined. Offline methods can include those that analyse the product (\ie, the translations) or verbal-report data (such as retrospective questionnaires). Online methods can include behaviour observation (eye tracking, keylogging, screen recording, etc.) and verbal-report data (such as think-aloud protocols, where translators say what they are doing while they are doing it). A detailed description of each is outside of the skope of this thesis, but the differences mainly relate to how much each method affects and can potentially alter the process it's trying to observe and how reliable the data gathered is. To overcome the potential downfalls of any one method, triangulation is used. Triangulation \parencite{alves2003triangulating} means collecting data using more than one of the methods previously described in order to contrast and complement the obtained data, gaining a better view of the overall picture.

Many recent studies have used \ac{TPR} to study translation and translation tools such as translation memories and \acl{PE}. Keylogging and eyetracking \parencite{obrien2009eyetracking} have been particularly popular. Databases have been created in an effort to standardise the collected data and allow for experiments on large datasets to be carried out, such as the \spacedlowsmallcaps{CRITT TPR-DB} \parencite{carl2012critt}. This thesis will follow in these footsteps at a smaller scale using some of the more basic \ac{TPR} methods (those that don't require specialised hardware), as will be discussed in \autoref{ch:experiment}.
